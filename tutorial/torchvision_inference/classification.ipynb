{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41a5cbbb-8d20-49ae-9dcc-90ed3f623793",
   "metadata": {},
   "source": [
    "### Classification example inference with Ryzen AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777199ee-9183-40bb-8355-1fb470552f5f",
   "metadata": {},
   "source": [
    "This example demonstrates the 5 steps of classification model inference on the embedded Neural Processing Unit (NPU) in your AMD Ryzen AI enabled PC. The steps are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8184db-eee4-499b-b5c0-c494e643bd06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import subprocess\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import onnx\n",
    "import shutil\n",
    "import time \n",
    "from timeit import default_timer as timer\n",
    "from quark.onnx import ModelQuantizer  \n",
    "from quark.onnx.quantization.config import Config, get_default_config  \n",
    "from utils_custom import ImageDataReader, evaluate_onnx_model \n",
    "import json  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3470d814-78bb-481e-b512-d431d267c003",
   "metadata": {},
   "source": [
    "#### 1. Get Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd67743-027b-4988-a467-b4a4a173259e",
   "metadata": {},
   "source": [
    "Here, we'll use the resnet50 model as an example. You may choose any classification models train with Imagenet from torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738304be-b948-4f34-a69d-ba4d2277e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Model Setup ---------------- #\n",
    "\n",
    "# Define directories\n",
    "models_dir = \"models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Load pre-trained ResNet50 model\n",
    "model = torchvision.models.resnet50(weights=\"IMAGENET1K_V2\")\n",
    "\n",
    "# Save the model\n",
    "model.to(\"cpu\")\n",
    "torch.save(model, os.path.join(models_dir, \"resnet50.pt\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce87c669-4f5c-433a-bb37-676b40d4640f",
   "metadata": {},
   "source": [
    "#### 2. Export to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f07e198-29b2-463a-b9df-10266d390249",
   "metadata": {},
   "source": [
    "The model inference with Ryzen AI is based on onnxruntime. The following code is used for exporting a PyTorch model to the ONNX (Open Neural Network Exchange) format. The ONNX file is needed to use the AMD Quark Quantizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d3bad8-b5ea-4c08-bea6-5c92c973055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model to ONNX\n",
    "dummy_inputs = torch.randn(1, 3, 224, 224)\n",
    "input_names = ['input']\n",
    "output_names = ['output']\n",
    "dynamic_axes = {'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "tmp_model_path = os.path.join(models_dir, \"resnet50.onnx\")\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_inputs,\n",
    "    tmp_model_path,\n",
    "    export_params=True,\n",
    "    opset_version=13,  # Recommended opset\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    dynamic_axes=dynamic_axes,\n",
    ")\n",
    "\n",
    "print(f\"✅ Model exported to ONNX at: {tmp_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad6bfea-4270-4d2d-98cd-0a5497224265",
   "metadata": {},
   "source": [
    "#### 3. Quantize Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3d5103-9e41-43c0-8298-fc4e5bd8d6d5",
   "metadata": {},
   "source": [
    "Using the AMD Quark Quantizer and providing the newly exported ONNX model, we'll quantize the model. The quantization progress will need the calibration data from Imagenet. Download the data from [here](https://huggingface.co/datasets/imagenet-1k/tree/main/data) to download it.\n",
    "You need to register on Hugging Face and download the following file:\n",
    "**val_images.tar.gz**.\n",
    "This file contains a subset of ImageNet images used specifically for calibration.\n",
    "\n",
    "Once downloaded, move the file to your working directory and extract the dataset into the calib_data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ae056f-a96c-47fb-b447-9de43e36e8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Quark Quantization ---------------- #\n",
    "\n",
    "# Define dataset directory\n",
    "calib_dir = \"calib_data\" \n",
    "\n",
    "# Set input & output ONNX model paths\n",
    "input_model_path = tmp_model_path\n",
    "output_model_path = os.path.join(models_dir, \"resnet50_quantized.onnx\")\n",
    "\n",
    "# Preprocessing transformations\n",
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "calib_dataset = torchvision.datasets.ImageFolder(root=calib_dir, transform=preprocess)\n",
    "\n",
    "#Data set \n",
    "num_calib_data = 54  \n",
    "calib_dataset = torch.utils.data.Subset(calib_dataset, range(num_calib_data))\n",
    "\n",
    "# Define DataLoader for Calibration\n",
    "calibration_dataloader = torch.utils.data.DataLoader(calib_dataset, batch_size=6, shuffle=False)\n",
    "\n",
    "# Configure Quark Quantization\n",
    "quant_config = get_default_config(\"XINT8\")  # Use XINT8 quantization  \n",
    "config = Config(global_quant_config=quant_config)\n",
    "\n",
    "# Create an ONNX Quantizer  \n",
    "quantizer = ModelQuantizer(config)  \n",
    "\n",
    "# Perform Quark Quantization  \n",
    "quant_model = quantizer.quantize_model(\n",
    "    model_input=input_model_path,   \n",
    "    model_output=output_model_path,   \n",
    "    calibration_data_reader=ImageDataReader(calibration_dataloader)  # Use ImageDataReader from utils_custom\n",
    ")\n",
    "\n",
    "print(f\"✅ Quark Quantized model saved at: {output_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46397081-c10c-4630-8299-563e81dea7b6",
   "metadata": {},
   "source": [
    "#### 4. Model inference on CPU / iGPU / NPU with single image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5cb104-5c6f-4043-a812-b5ee4a48af57",
   "metadata": {},
   "source": [
    "Now we have successfully quantized the model, and we will use the onnxruntime to do the inference on CPU, iGPU and NPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98ec05e-9b2e-4cc0-b3af-6d640c167074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def load_labels(path):\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "    return np.asarray(data)\n",
    "\n",
    "def preprocess_image(input):\n",
    "    normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "  \n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Resize((224, 224)),\n",
    "        normalize,\n",
    "    ])\n",
    "    img_tensor = transform(input).unsqueeze(0)\n",
    "    return img_tensor.numpy()\n",
    "\n",
    "def softmax(x):\n",
    "    x = x.reshape(-1)\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def postprocess(result):\n",
    "    return softmax(np.array(result)).tolist()\n",
    "\n",
    "labels = load_labels('data/imagenet-simple-labels.json')\n",
    "image = Image.open('data/dog.jpg')\n",
    "\n",
    "print(\"Image size: \", image.size)\n",
    "input_data = preprocess_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db896e-fe9b-4167-9b0e-758040d50dd4",
   "metadata": {},
   "source": [
    "#### CPU Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd83dc38-b0ad-4001-bb8f-31b6b6f0cc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on CPU\n",
    "onnx_model_path = output_model_path\n",
    "cpu_options = onnxruntime.SessionOptions()\n",
    "\n",
    "cpu_session = onnxruntime.InferenceSession(\n",
    "    onnx_model_path,\n",
    "    providers=['CPUExecutionProvider'],\n",
    "    sess_options=cpu_options,\n",
    ")\n",
    "\n",
    "start = timer()\n",
    "cpu_outputs = cpu_session.run(None, {'input': input_data})\n",
    "end = timer()\n",
    "\n",
    "cpu_results = postprocess(cpu_outputs)\n",
    "inference_time = np.round((end - start) * 1000, 2)\n",
    "idx = np.argmax(cpu_results)\n",
    "\n",
    "print('----------------------------------------')\n",
    "print(f'Final top prediction is: {labels[idx]}')\n",
    "print('----------------------------------------')\n",
    "print(f'Inference time: {inference_time} ms')\n",
    "print('----------------------------------------')\n",
    "\n",
    "sort_idx = np.flip(np.squeeze(np.argsort(cpu_results)))\n",
    "print('------------ Top 5 labels are: ----------------------------')\n",
    "print(labels[sort_idx[:5]])\n",
    "print('-----------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d267505-2078-44f0-9db2-e150b6e7af76",
   "metadata": {},
   "source": [
    "#### iGPU Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b83ff1-63a1-40cf-b4a5-013857b42a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iGPU inference\n",
    "dml_options = onnxruntime.SessionOptions()\n",
    "\n",
    "# Create Inference Session to run the quantized model on the iGPU\n",
    "dml_session = onnxruntime.InferenceSession(\n",
    "    onnx_model_path,\n",
    "    providers = ['DmlExecutionProvider'],\n",
    "    provider_options = [{\"device_id\": \"0\"}]\n",
    ")\n",
    "start = time.time()\n",
    "dml_outputs = dml_session.run(None, {'input': input_data})\n",
    "end = time.time()\n",
    "\n",
    "dml_results = postprocess(dml_outputs)\n",
    "inference_time = np.round((end - start) * 1000, 2)\n",
    "idx = np.argmax(dml_results)\n",
    "\n",
    "print('----------------------------------------')\n",
    "print('Final top prediction is: ' + labels[idx])\n",
    "print('----------------------------------------')\n",
    "\n",
    "print('----------------------------------------')\n",
    "print('Inference time: ' + str(inference_time) + \" ms\")\n",
    "print('----------------------------------------')\n",
    "\n",
    "sort_idx = np.flip(np.squeeze(np.argsort(dml_results)))\n",
    "print('------------ Top 5 labels are: ----------------------------')\n",
    "print(labels[sort_idx[:5]])\n",
    "print('-----------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c9156d-6186-4256-9e3d-3826d6a3e0b8",
   "metadata": {},
   "source": [
    "#### NPU Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c122300c-c9c7-43b7-8e58-38a053a7f227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NPU inference\n",
    "\n",
    "# Before running, we need to set the ENV variable for the specific NPU we have\n",
    "# Run pnputil as a subprocess to enumerate PCI devices\n",
    "command = r'pnputil /enum-devices /bus PCI /deviceids '\n",
    "process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "stdout, stderr = process.communicate()\n",
    "# Check for supported Hardware IDs\n",
    "apu_type = ''\n",
    "if 'PCI\\\\VEN_1022&DEV_1502&REV_00' in stdout.decode(): apu_type = 'PHX/HPT'\n",
    "if 'PCI\\\\VEN_1022&DEV_17F0&REV_00' in stdout.decode(): apu_type = 'STX'\n",
    "if 'PCI\\\\VEN_1022&DEV_17F0&REV_10' in stdout.decode(): apu_type = 'STX'\n",
    "if 'PCI\\\\VEN_1022&DEV_17F0&REV_11' in stdout.decode(): apu_type = 'STX'\n",
    "\n",
    "print(f\"APU Type: {apu_type}\")\n",
    "\n",
    "install_dir = os.environ['RYZEN_AI_INSTALLATION_PATH']\n",
    "match apu_type:\n",
    "    case 'PHX/HPT':\n",
    "        print(\"Setting environment for PHX/HPT\")\n",
    "        os.environ['XLNX_VART_FIRMWARE']= os.path.join(install_dir, 'voe-4.0-win_amd64', 'xclbins', 'phoenix', '1x4.xclbin')\n",
    "        os.environ['NUM_OF_DPU_RUNNERS']='1'\n",
    "        os.environ['XLNX_TARGET_NAME']='AMD_AIE2_Nx4_Overlay'\n",
    "    case 'STX':\n",
    "        print(\"Setting environment for STX\")\n",
    "        os.environ['XLNX_VART_FIRMWARE']= os.path.join(install_dir, 'voe-4.0-win_amd64', 'xclbins', 'strix', 'AMD_AIE2P_Nx4_Overlay.xclbin')\n",
    "        os.environ['NUM_OF_DPU_RUNNERS']='1'\n",
    "        os.environ['XLNX_TARGET_NAME']='AMD_AIE2_Nx4_Overlay'\n",
    "    case _:\n",
    "        print(\"Unrecognized APU type. Exiting.\")\n",
    "        exit()\n",
    "print('XLNX_VART_FIRMWARE=', os.environ['XLNX_VART_FIRMWARE'])\n",
    "print('NUM_OF_DPU_RUNNERS=', os.environ['NUM_OF_DPU_RUNNERS'])\n",
    "print('XLNX_TARGET_NAME=', os.environ['XLNX_TARGET_NAME'])\n",
    "\n",
    "\n",
    "## Point to the config file path used for the VitisAI Execution Provider\n",
    "config_file_path = \"./vaip_config.json\"\n",
    "provider_options = [{\n",
    "              'config_file': config_file_path,\n",
    "              'ai_analyzer_visualization': True,\n",
    "              'ai_analyzer_profiling': True,\n",
    "          }]\n",
    "\n",
    "npu_session = onnxruntime.InferenceSession(\n",
    "    onnx_model_path,\n",
    "    providers = ['VitisAIExecutionProvider'],\n",
    "    provider_options = provider_options\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "npu_outputs = npu_session.run(None, {'input': input_data})\n",
    "end = time.time()\n",
    "\n",
    "npu_results = postprocess(npu_outputs)\n",
    "inference_time = np.round((end - start) * 1000, 2)\n",
    "idx = np.argmax(npu_results)\n",
    "\n",
    "print('----------------------------------------')\n",
    "print('Final top prediction is: ' + labels[idx])\n",
    "print('----------------------------------------')\n",
    "\n",
    "print('----------------------------------------')\n",
    "print('Inference time: ' + str(inference_time) + \" ms\")\n",
    "print('----------------------------------------')\n",
    "\n",
    "sort_idx = np.flip(np.squeeze(np.argsort(npu_results)))\n",
    "print('------------ Top 5 labels are: ----------------------------')\n",
    "print(labels[sort_idx[:5]])\n",
    "print('-----------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b1bae-6afa-434a-9393-373a5aad3ac9",
   "metadata": {},
   "source": [
    "#### 5. Model Analysis on NPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee5f148-094f-4e91-87c7-bce623a5af2e",
   "metadata": {},
   "source": [
    "After NPU inference, there are several '.json' files generated by the Ryzen AI tracing tool, which could be open by the AI Analyzer for further optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb04c788-f16a-4ce5-a7e8-6e414a45fda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aianalyzer ./ -p 8001"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ryzen-ai-1.3.1",
   "language": "python",
   "name": "ryzen-ai-1.3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
